{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53e339-bcb7-498f-933f-c9f744fba8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1.\n",
    "\n",
    "GridSearchCV (Grid Search Cross-Validation) is a technique used in machine learning for hyperparameter tuning, which is the process of finding the optimal values for the hyperparameters of a machine learning model. The purpose of GridSearchCV is to systematically search through a predefined hyperparameter grid and determine the best combination of hyperparameter values that yields the highest performance for a given model.\n",
    "\n",
    "Here's how GridSearchCV works:\n",
    "\n",
    "Define the Model: First, you need to define the machine learning model you want to tune, along with the range of values for the hyperparameters you want to optimize.\n",
    "\n",
    "Define the Hyperparameter Grid: Create a dictionary where the keys are the names of the hyperparameters, and the values are the lists of values to be tested. Each combination of the values from all the hyperparameters will be evaluated.\n",
    "\n",
    "Cross-Validation: Specify the cross-validation strategy to evaluate the performance of each hyperparameter combination. Typically, k-fold cross-validation is used, where the data is divided into k equal-sized subsets (folds), and each fold is used as a validation set while the remaining k-1 folds are used for training. This process is repeated for each hyperparameter combination.\n",
    "\n",
    "Perform Grid Search: GridSearchCV exhaustively searches through all possible combinations of hyperparameter values by fitting the model on the training data and evaluating it on the validation data for each combination.\n",
    "\n",
    "Model Selection: Once the grid search is complete, GridSearchCV selects the hyperparameter combination that achieved the highest performance based on a specified evaluation metric, such as accuracy, precision, recall, or F1-score.\n",
    "\n",
    "Evaluate on Test Set: Finally, the selected model with the best hyperparameters is evaluated on an independent test set to estimate its generalization performance.\n",
    "\n",
    "GridSearchCV automates the process of hyperparameter tuning and provides a systematic way to find the best hyperparameters for a given machine learning model. By exhaustively searching through a predefined hyperparameter grid, it helps in optimizing the model's performance and avoiding manual trial-and-error tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09908bb1-1726-4bf9-89fb-648a8c31a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.\n",
    "\n",
    "GridSearchCV and RandomizedSearchCV are both techniques used for hyperparameter tuning in machine learning. While they serve a similar purpose, there are key differences between the two approaches:\n",
    "\n",
    "Search Strategy:\n",
    "\n",
    "GridSearchCV: Grid search systematically searches through all possible combinations of hyperparameter values specified in a predefined grid. It exhaustively evaluates each combination, making it computationally expensive when the hyperparameter search space is large.\n",
    "RandomizedSearchCV: Randomized search, on the other hand, randomly samples a fixed number of combinations from the hyperparameter search space. It doesn't evaluate all possible combinations, making it more efficient for larger search spaces. Random sampling allows exploration of a wider range of values, which can be useful when the impact of individual hyperparameters is not clear.\n",
    "Search Space Exploration:\n",
    "\n",
    "GridSearchCV: Grid search explores the entire search space defined by the grid. It evaluates all combinations within the specified grid, ensuring that no combination is missed. This can be advantageous when you have prior knowledge or domain expertise to narrow down the search space.\n",
    "RandomizedSearchCV: Randomized search explores a random subset of the search space. It randomly selects combinations for evaluation, which allows for a broader exploration of the hyperparameter space. This can be beneficial when you have limited knowledge about the hyperparameters or when the search space is large and exhaustive evaluation is impractical.\n",
    "Computation Time:\n",
    "\n",
    "GridSearchCV: Since grid search evaluates all possible combinations, it can be computationally expensive, especially when dealing with a large number of hyperparameters and their possible values. The search time grows exponentially with the number of hyperparameters and their potential values.\n",
    "RandomizedSearchCV: Randomized search samples a fixed number of combinations, making it more time-efficient than grid search. It is particularly useful when you have limited computational resources or when the search space is large.\n",
    "When to choose one over the other:\n",
    "\n",
    "Choose GridSearchCV when:\n",
    "\n",
    "The hyperparameter search space is relatively small.\n",
    "You have some prior knowledge or a specific intuition about the optimal hyperparameter values.\n",
    "You have sufficient computational resources to exhaustively evaluate all combinations.\n",
    "Choose RandomizedSearchCV when:\n",
    "\n",
    "The hyperparameter search space is large and evaluating all combinations is impractical.\n",
    "You have limited prior knowledge or uncertain about the optimal hyperparameter values.\n",
    "You have limited computational resources and need a more time-efficient approach.\n",
    "In summary, GridSearchCV performs an exhaustive search of the specified grid, while RandomizedSearchCV randomly samples combinations. Grid search is suitable for smaller search spaces or when you have specific knowledge, while randomized search is efficient for larger search spaces or when you have limited knowledge about the optimal hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa000ea9-d48e-4dbb-8245-297db5f52e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3.\n",
    "\n",
    "Data leakage refers to the situation where information from outside the training data is inadvertently used to create or evaluate a machine learning model, leading to an overly optimistic performance estimation. It occurs when there is a \"leak\" of information from the validation or test set into the training process, violating the assumption that the model is being trained and evaluated on independent and identically distributed (i.i.d.) data.\n",
    "\n",
    "Data leakage can be problematic in machine learning for several reasons:\n",
    "\n",
    "Overestimated Performance: When data leakage occurs, the model's performance on the training, validation, or test set becomes overly optimistic. It may lead to inflated accuracy or other evaluation metrics, giving a false sense of the model's true capabilities. This can result in models that fail to generalize well to new, unseen data.\n",
    "\n",
    "Invalidating Model Assessment: Data leakage can invalidate the assessment of the model's performance. If the model has access to information it should not have during training or evaluation, the performance metrics no longer accurately reflect the model's ability to generalize to real-world scenarios.\n",
    "\n",
    "Misleading Feature Importance: Data leakage can lead to misleading feature importance rankings. Features that leak information from the target variable may appear highly significant in the model, even though they are not truly predictive in a generalizable sense.\n",
    "\n",
    "Example of Data Leakage:\n",
    "Let's consider an example of a credit card fraud detection model. Suppose the dataset contains information about credit card transactions, including transaction amounts and whether they are fraudulent or not. The goal is to build a model that accurately predicts fraudulent transactions.\n",
    "\n",
    "Data leakage occurs when the model uses features that are derived from information not available at the time of the transaction. For instance, if the dataset includes the transaction timestamp and the model uses this feature, it may inadvertently learn that transactions occurring at certain times of the day are more likely to be fraudulent. However, in real-world scenarios, the model would not have access to the timestamp at the time of prediction, rendering this feature irrelevant and introducing data leakage.\n",
    "\n",
    "In this case, the model's performance would be overestimated during training and evaluation since it has access to future information (i.e., the timestamp) that it would not have in a real-world scenario. This can lead to a false sense of accuracy and the model failing to perform well when deployed in practice.\n",
    "\n",
    "To mitigate data leakage, it is crucial to carefully analyze the data and ensure that only features that are available and relevant at the time of prediction are used during model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9b05d-def7-4af3-b74f-194c63b2a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4.\n",
    "\n",
    "Preventing data leakage is essential to ensure the integrity and generalization performance of machine learning models. Here are several best practices to prevent data leakage when building a machine learning model:\n",
    "\n",
    "Separate Data Properly:\n",
    "\n",
    "Train-Validation-Test Split: Split your dataset into three separate sets: a training set, a validation set, and a test set. The training set is used for model training, the validation set is used for hyperparameter tuning and model evaluation, and the test set is used for final model evaluation. Make sure these sets are mutually exclusive and do not overlap.\n",
    "\n",
    "Time-Based Split: If your dataset has a time component, ensure that the data is split chronologically. For example, use earlier time periods for training, intermediate periods for validation, and the latest periods for testing. This approach simulates the real-world scenario where the model is trained on past data and tested on future data.\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "Use only Available Features: Make sure to exclude any features that are not available at the time of prediction. For example, if building a model to predict future stock prices, features like future prices or economic indicators that are not known at prediction time should be excluded.\n",
    "\n",
    "Be Mindful of Feature Generation: Avoid creating features derived from the target variable or features that leak information about the target. Ensure that feature engineering operations are based solely on information available before the prediction is made.\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Use Proper Cross-Validation Techniques: When performing cross-validation, such as k-fold cross-validation, ensure that the data splits are done correctly. Each fold should represent an independent subset of the data, and the validation set for each fold should not contain any instances that are part of the training set.\n",
    "Domain Knowledge:\n",
    "\n",
    "Understand the Problem Domain: Develop a good understanding of the problem domain and the data you are working with. This will help you identify potential sources of data leakage and make informed decisions about feature selection and preprocessing.\n",
    "Data Pipeline:\n",
    "\n",
    "Implement a Pipeline: Use a data pipeline to ensure that all preprocessing steps, including feature engineering and scaling, are applied separately to the training, validation, and test sets. This ensures that data transformations are performed consistently across different datasets and prevents leakage from preprocessing steps.\n",
    "Continuous Monitoring:\n",
    "\n",
    "Monitor Model Performance: Continuously monitor the performance of your model in real-world applications. Regularly evaluate the model's performance on new data and compare it with the performance observed during development and validation. This can help identify unexpected issues or potential sources of data leakage.\n",
    "By following these preventive measures, you can minimize the risk of data leakage and ensure that your machine learning models are built and evaluated in a robust and reliable manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663fffb-772b-46b1-b73f-eb1ba2ee8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5.\n",
    "\n",
    "A confusion matrix, also known as an error matrix, is a table that summarizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions. It is a useful tool for evaluating the performance and understanding the behavior of a classification model.\n",
    "\n",
    "A confusion matrix is typically organized in a 2x2 table for binary classification problems, but it can also be extended to multi-class problems.\n",
    "\n",
    "Here's a breakdown of the elements in a binary classification confusion matrix:\n",
    "\n",
    "True Positive (TP): The model correctly predicted the positive class.\n",
    "True Negative (TN): The model correctly predicted the negative class.\n",
    "False Positive (FP): The model incorrectly predicted the positive class (Type I error).\n",
    "False Negative (FN): The model incorrectly predicted the negative class (Type II error).\n",
    "The confusion matrix provides the following important metrics for assessing the performance of a classification model:\n",
    "\n",
    "Accuracy: The overall accuracy of the model is calculated as (TP + TN) / (TP + TN + FP + FN). It represents the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    "Precision: Precision is the measure of how many predicted positive instances are actually positive. It is calculated as TP / (TP + FP). Precision focuses on the positive class and helps evaluate the model's ability to avoid false positives.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of actual positive instances that are correctly predicted. It is calculated as TP / (TP + FN). Recall focuses on the positive class and helps evaluate the model's ability to avoid false negatives.\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures the proportion of actual negative instances that are correctly predicted. It is calculated as TN / (TN + FP). Specificity focuses on the negative class and helps evaluate the model's ability to avoid false positives.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of the model's accuracy by considering both false positives and false negatives. The F1-score is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "By examining these metrics from the confusion matrix, you can gain insights into the strengths and weaknesses of the classification model. For example, a high accuracy, precision, recall, and F1-score indicate a well-performing model, while imbalances or low values in these metrics can highlight issues such as misclassifications, bias, or overfitting. The confusion matrix provides a more detailed understanding of the model's performance beyond a single accuracy score and helps guide improvements and decision-making related to the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34e7c2-0c84-45b6-bcde-8e4e2d5b3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6.\n",
    "\n",
    "Precision and recall are two important metrics used to evaluate the performance of a classification model, and they are calculated based on the values in the confusion matrix.\n",
    "\n",
    "In the context of a confusion matrix, precision and recall represent different aspects of the model's prediction performance:\n",
    "\n",
    "Precision: Precision measures the proportion of correctly predicted positive instances out of all instances that the model predicted as positive (true positives and false positives). It focuses on the positive predictions made by the model.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Precision provides insights into how well the model avoids false positives. A high precision indicates that the model has a low rate of falsely predicting positive instances, suggesting that when it predicts positive, it is likely to be correct.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of correctly predicted positive instances out of all actual positive instances (true positives and false negatives). It focuses on the actual positive instances in the dataset.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "Recall provides insights into how well the model avoids false negatives. A high recall indicates that the model has a low rate of falsely predicting negative instances, suggesting that it is effective at capturing most of the positive instances in the dataset.\n",
    "\n",
    "To further illustrate the difference, consider the following scenarios:\n",
    "\n",
    "High Precision, Low Recall: In this scenario, the model predicts positive sparingly, but when it does, it is often correct. However, it may miss a significant number of actual positive instances, resulting in a low recall. This could be the case when the model prioritizes avoiding false positives at the cost of potentially missing some positive instances.\n",
    "\n",
    "High Recall, Low Precision: In this scenario, the model predicts positive more frequently, capturing most of the actual positive instances. However, it may also make a substantial number of false positive predictions, resulting in a low precision. This could be the case when the model is sensitive and tends to classify many instances as positive, potentially including some that are actually negative.\n",
    "\n",
    "The choice between optimizing for precision or recall depends on the specific problem and its associated costs and priorities. For example, in spam email detection, high precision is crucial to avoid falsely classifying legitimate emails as spam, while in disease diagnosis, high recall is often desired to minimize the risk of missing positive cases, even at the cost of some false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755355a-63dd-4e99-bf15-cacc9c51079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q7.\n",
    "\n",
    "Interpreting a confusion matrix allows you to understand the types of errors your classification model is making. By examining the values within the matrix, you can identify the specific types of misclassifications and gain insights into the model's performance. Here's how you can interpret a confusion matrix:\n",
    "\n",
    "True Positive (TP): This represents the number of instances that are correctly predicted as positive by the model. These are the instances that belong to the positive class, and the model correctly identifies them as positive.\n",
    "\n",
    "True Negative (TN): This indicates the number of instances that are correctly predicted as negative by the model. These instances belong to the negative class, and the model accurately identifies them as negative.\n",
    "\n",
    "False Positive (FP): This reflects the number of instances that are incorrectly predicted as positive by the model. These instances actually belong to the negative class, but the model wrongly classifies them as positive. It represents the Type I error or a false positive prediction.\n",
    "\n",
    "False Negative (FN): This signifies the number of instances that are incorrectly predicted as negative by the model. These instances belong to the positive class, but the model incorrectly classifies them as negative. It represents the Type II error or a false negative prediction.\n",
    "\n",
    "To interpret the confusion matrix and understand the errors made by the model, consider the following scenarios:\n",
    "\n",
    "High False Positives (FP): If the model has a high number of false positives, it means that it is incorrectly classifying negative instances as positive. This could indicate that the model is being too aggressive in predicting positive instances or that the negative class is not well separated from the positive class.\n",
    "\n",
    "High False Negatives (FN): If the model has a high number of false negatives, it means that it is incorrectly classifying positive instances as negative. This suggests that the model is missing important positive instances or that the positive class is not well captured by the model.\n",
    "\n",
    "Imbalanced Classes: If one class has significantly more instances than the other, it can influence the confusion matrix. For example, if the negative class has a much larger representation in the dataset, the model may perform well in terms of accuracy but may have poor recall for the positive class due to a higher tendency to predict negative.\n",
    "\n",
    "Analyzing the confusion matrix helps you understand the specific types of errors your model is making, allowing you to focus on areas that need improvement. It can guide you in adjusting the model's threshold, applying class weights, feature engineering, or exploring different algorithms to address the identified errors and enhance the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f39828-23fb-4bba-8d5a-9fa622d25344",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8.\n",
    "\n",
    "Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. Here are some key metrics and their calculations:\n",
    "\n",
    "Accuracy: Accuracy measures the overall correctness of the model's predictions.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "It represents the proportion of correctly classified instances out of the total number of instances.\n",
    "\n",
    "Precision: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It focuses on the positive predictions made by the model.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Precision evaluates the model's ability to avoid false positives and provides insights into the correctness of positive predictions.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. It focuses on the actual positive instances in the dataset.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "Recall evaluates the model's ability to avoid false negatives and provides insights into the completeness of positive predictions.\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures the proportion of correctly predicted negative instances out of all actual negative instances. It focuses on the actual negative instances in the dataset.\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "Specificity evaluates the model's ability to avoid false positives for the negative class.\n",
    "\n",
    "F1-Score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of the model's accuracy by considering both false positives and false negatives.\n",
    "\n",
    "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "The F1-score balances precision and recall, particularly useful when classes are imbalanced.\n",
    "\n",
    "False Positive Rate (FPR): FPR measures the proportion of actual negative instances that are incorrectly predicted as positive. It is the complement of specificity.\n",
    "\n",
    "FPR = 1 - Specificity\n",
    "\n",
    "FPR provides insights into the model's false positive predictions for the negative class.\n",
    "\n",
    "These metrics derived from the confusion matrix help evaluate different aspects of the model's performance, such as overall correctness, precision, recall, specificity, and the balance between precision and recall. They provide a comprehensive understanding of the model's strengths and weaknesses in making correct predictions and avoiding false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c1e29-420a-4c6c-82ef-3ccc1733a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q9.\n",
    "\n",
    "The accuracy of a model and the values in its confusion matrix are interconnected but provide different perspectives on the model's performance. The confusion matrix provides detailed information about the true positives, true negatives, false positives, and false negatives, while accuracy represents the overall correctness of the model's predictions.\n",
    "\n",
    "The relationship between accuracy and the values in the confusion matrix can be understood as follows:\n",
    "\n",
    "Accuracy Calculation: Accuracy is calculated as the ratio of correct predictions (true positives and true negatives) to the total number of predictions (all four values in the confusion matrix).\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Accuracy considers all prediction outcomes and provides a single metric to evaluate the model's overall correctness.\n",
    "\n",
    "Impact of Confusion Matrix Values on Accuracy:\n",
    "\n",
    "True Positives (TP) and True Negatives (TN): Increasing the number of true positives and true negatives will increase the accuracy since they are correct predictions.\n",
    "\n",
    "False Positives (FP) and False Negatives (FN): Increasing the number of false positives or false negatives will decrease the accuracy since they represent incorrect predictions.\n",
    "\n",
    "The accuracy metric treats all types of predictions equally and provides an overall assessment of the model's correctness without differentiating between the types of errors made.\n",
    "\n",
    "Accuracy Limitations: Accuracy alone may not provide a complete understanding of the model's performance, especially when dealing with imbalanced datasets or when the costs of different types of errors are not equal. In such cases, accuracy can be misleading because the model may appear to have high accuracy while performing poorly on specific classes or making critical misclassifications.\n",
    "\n",
    "Therefore, it is essential to consider additional metrics derived from the confusion matrix, such as precision, recall, specificity, and F1-score, to gain a more comprehensive evaluation of the model's performance.\n",
    "\n",
    "In summary, the accuracy of a model is influenced by the values in its confusion matrix, with true positives and true negatives contributing to higher accuracy, while false positives and false negatives lower the accuracy. However, accuracy alone may not provide a complete picture of the model's performance, and it is important to consider the specific errors made by the model using metrics derived from the confusion matrix to gain deeper insights into its strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491d016-90b6-440b-8594-18d1c2a5fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q10.\n",
    "\n",
    "A confusion matrix can be a valuable tool for identifying potential biases or limitations in a machine learning model. By examining the values within the confusion matrix, you can gain insights into the model's performance across different classes and identify areas where biases or limitations may exist. Here are some approaches to utilize the confusion matrix for this purpose:\n",
    "\n",
    "Class Imbalance: Check if the dataset used for training the model is imbalanced, meaning that one class has significantly more instances than the others. This can lead to biased predictions, as the model might favor the majority class. If there is a severe class imbalance, it is important to consider additional evaluation metrics beyond accuracy to assess the model's performance accurately.\n",
    "\n",
    "False Positive and False Negative Rates: Look at the false positive and false negative rates across different classes. High false positive rates indicate a tendency to incorrectly predict instances from a particular class as positive, while high false negative rates suggest a tendency to miss instances from a particular class. These discrepancies can be indicative of bias or limitations in the model's ability to handle certain classes.\n",
    "\n",
    "Error Analysis: Examine the specific instances that are misclassified in the confusion matrix. This analysis can help identify patterns or common characteristics of misclassified instances, especially for classes where the model struggles. By understanding the reasons behind the misclassifications, you can gain insights into the limitations of the model and potential biases present in the data.\n",
    "\n",
    "Disparate Impact: Consider the performance metrics (e.g., precision, recall, and F1-score) for different classes, particularly when dealing with sensitive attributes like gender or race. If the model shows significant differences in performance across different groups, it could indicate potential biases. Analyzing the confusion matrix in the context of protected attributes can help identify whether the model's predictions disproportionately favor or disfavor certain groups.\n",
    "\n",
    "External Factors: Take into account external factors that might influence the model's performance. For example, if the model is trained on data from a specific region or time period, it may not generalize well to different regions or future scenarios. Analyzing the confusion matrix can reveal limitations in the model's ability to handle variations or changes in the data distribution.\n",
    "\n",
    "By utilizing the insights from the confusion matrix, you can identify potential biases or limitations in your machine learning model. This understanding can help guide further investigation, model improvements, feature engineering, or the inclusion of additional data to address the identified biases or limitations and improve the fairness and robustness of the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
